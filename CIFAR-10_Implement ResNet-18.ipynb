{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework8_2018311840_강승구.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdpFT5R_etqi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "41ff96bb-ee52-4898-8e67-ca4c89388e63"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#dataset\n",
        "trtransform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Normalize the test set same as training set without augmentation\n",
        "tetransform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=trtransform)\n",
        "test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=tetransform)\n",
        "\n",
        "num = len(train)\n",
        "indices = list(range(num))\n",
        "split = int(np.floor(0.1 * num))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "np.random.seed(10)\n",
        "np.random.shuffle(indices)\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train, batch_size=128, sampler=train_sampler)\n",
        "validloader = torch.utils.data.DataLoader(train, batch_size=128, sampler=valid_sampler)\n",
        "testloader = torch.utils.data.DataLoader(test, batch_size=128, shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "  def __init__(self, planes):\n",
        "    super(SEBlock, self).__init__()\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "    self.fc = nn.Sequential(nn.Linear(planes, planes // 16), nn.ReLU(inplace=True), nn.Linear(planes // 16, planes), nn.Sigmoid())\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, c, _, _ = x.size()\n",
        "    y = self.avg_pool(x).view(b, c)\n",
        "    y = self.fc(y).view(b, c, 1, 1)\n",
        "    return x * y.expand_as(x)\n",
        "      \n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, stride):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.senet = SEBlock(planes)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = self.senet(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "      \n",
        "class resnet18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(resnet18, self).__init__()\n",
        "        self.in_planes = 16\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv1_bn = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(BasicBlock, 16, 3, stride=1)\n",
        "        self.layer2 = self._make_layer(BasicBlock, 32, 3, stride=2)\n",
        "        self.layer3 = self._make_layer(BasicBlock, 64, 3, stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.linear = nn.Linear(64, 10)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1_bn(self.conv1(x)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "model = resnet18().to(device)\n",
        "\n",
        "#hyperparameter\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
        "\n",
        "#Train\n",
        "for epoch in range(1,51):\n",
        "  model.train()\n",
        "  train_loss = 0.0\n",
        "  val_loss=0.0\n",
        "  traincorrect=0\n",
        "  valcorrect = 0\n",
        "  for image, label in trainloader:\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    output = model(image)\n",
        "    loss = criterion(output, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item() / 45000\n",
        "    _, pred = torch.max(output.data, 1)\n",
        "    traincorrect += (pred == label).sum().item()\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for image, label in validloader:\n",
        "      image, label = image.to(device), label.to(device)\n",
        "      output = model(image)\n",
        "      loss = criterion(output, label)\n",
        "      val_loss += loss.item() / 5000\n",
        "      _, pred = torch.max(output.data, 1)\n",
        "      valcorrect += (pred == label).sum().item()\n",
        "  scheduler.step()\n",
        "  if epoch % 5 == 0 :\n",
        "    print('Epoch:{}/{}, train_loss:{}, train_acc:{}, val_loss:{}, validation Accuracy: {}'.format(epoch, 50, train_loss, traincorrect/45000, val_loss, 100 * valcorrect / 5000))\n",
        "  \n",
        "\n",
        "#Test acc\n",
        "correct = 0\n",
        "tot = 0\n",
        "with torch.no_grad():\n",
        "  for image, label in testloader:\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    output = model(image)\n",
        "    _, pred = torch.max(output.data, 1)\n",
        "    correct += (pred == label).sum().item()\n",
        "\n",
        "print('Test Accuracy: {}'.format(100 * correct / len(testloader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}