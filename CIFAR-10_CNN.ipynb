{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework6_2018311840_강승구.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fce6Kxv3v1KX",
        "colab_type": "code",
        "outputId": "ce3d079a-11d6-4126-9a71-c6d55a96c19b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        }
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#dataset\n",
        "trtransform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Normalize the test set same as training set without augmentation\n",
        "tetransform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=trtransform)\n",
        "test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=tetransform)\n",
        "\n",
        "num = len(train)\n",
        "indices = list(range(num))\n",
        "split = int(np.floor(0.1 * num))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "np.random.seed(10)\n",
        "np.random.shuffle(indices)\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train, batch_size=64, sampler=train_sampler)\n",
        "validloader = torch.utils.data.DataLoader(train, batch_size=64, sampler=valid_sampler)\n",
        "testloader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv1_bn = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2_bn = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3_bn = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.dropout1 = nn.Dropout2d(p=0.05)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 120)\n",
        "        self.fc1_bn = nn.BatchNorm1d(120)\n",
        "        self.fc2 = nn.Linear(120, 60)\n",
        "        self.fc2_bn = nn.BatchNorm1d(60)\n",
        "        self.fc3 = nn.Linear(60, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1_bn(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.conv2_bn(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.conv3_bn(self.conv3(x))))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = x.view(-1, 128 *4 * 4)\n",
        "        x = F.relu(self.fc1_bn(self.fc1(x)))\n",
        "        x = F.relu(self.fc2_bn(self.fc2(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = Net().to(device)\n",
        "\n",
        "#hyperparameter\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
        "min_val_loss = 1\n",
        "count=0\n",
        "\n",
        "#Train\n",
        "for epoch in range(1,101):\n",
        "  model.train()\n",
        "  train_loss = 0.0\n",
        "  val_loss=0.0\n",
        "  correct = 0\n",
        "  for image, label in trainloader:\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    output = model(image)\n",
        "    loss = criterion(output, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "  \n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for image, label in validloader:\n",
        "      image, label = image.to(device), label.to(device)\n",
        "      output = model(image)\n",
        "      loss = criterion(output, label)\n",
        "      val_loss += loss.item() / 5000\n",
        "      _, pred = torch.max(output.data, 1)\n",
        "      correct += (pred == label).sum().item()\n",
        "  print('Epoch:{}/{}, train_loss:{}, validation Accuracy: {}'.format(epoch, 100, train_loss / 45000, 100 * correct / 5000))\n",
        "  scheduler.step()\n",
        "  if val_loss < min_val_loss:\n",
        "    count=0\n",
        "    min_val_loss = val_loss\n",
        "  else:\n",
        "    count+=1\n",
        "  if count == 7:\n",
        "    print(\"Early Stopped!\")\n",
        "    break\n",
        "\n",
        "#Test acc\n",
        "correct = 0\n",
        "tot = 0\n",
        "with torch.no_grad():\n",
        "  for image, label in testloader:\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    output = model(image)\n",
        "    _, pred = torch.max(output.data, 1)\n",
        "    correct += (pred == label).sum().item()\n",
        "\n",
        "print('Test Accuracy: {}'.format(100 * correct / len(testloader.dataset)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch:1/100, train_loss:0.026910161736276413, validation Accuracy: 41.6\n",
            "Epoch:2/100, train_loss:0.021499770487679374, validation Accuracy: 50.76\n",
            "Epoch:3/100, train_loss:0.018492825645870632, validation Accuracy: 59.8\n",
            "Epoch:4/100, train_loss:0.01671929239961836, validation Accuracy: 62.48\n",
            "Epoch:5/100, train_loss:0.015444826420148213, validation Accuracy: 69.16\n",
            "Epoch:6/100, train_loss:0.01447360364596049, validation Accuracy: 68.4\n",
            "Epoch:7/100, train_loss:0.013588668402036031, validation Accuracy: 70.66\n",
            "Epoch:8/100, train_loss:0.012979773398902682, validation Accuracy: 72.06\n",
            "Epoch:9/100, train_loss:0.012333800266186397, validation Accuracy: 74.0\n",
            "Epoch:10/100, train_loss:0.011831282095776665, validation Accuracy: 73.44\n",
            "Epoch:11/100, train_loss:0.011219521139065424, validation Accuracy: 75.48\n",
            "Epoch:12/100, train_loss:0.010872043123510148, validation Accuracy: 77.7\n",
            "Epoch:13/100, train_loss:0.010553104105922912, validation Accuracy: 77.7\n",
            "Epoch:14/100, train_loss:0.010211874992979897, validation Accuracy: 78.58\n",
            "Epoch:15/100, train_loss:0.009851171890232298, validation Accuracy: 79.18\n",
            "Epoch:16/100, train_loss:0.009473576890097724, validation Accuracy: 78.76\n",
            "Epoch:17/100, train_loss:0.009252429019742542, validation Accuracy: 78.72\n",
            "Epoch:18/100, train_loss:0.009140874446431796, validation Accuracy: 79.82\n",
            "Epoch:19/100, train_loss:0.008819769207636515, validation Accuracy: 80.24\n",
            "Epoch:20/100, train_loss:0.008643909914957152, validation Accuracy: 81.08\n",
            "Epoch:21/100, train_loss:0.008271233830518194, validation Accuracy: 82.2\n",
            "Epoch:22/100, train_loss:0.008202092032300101, validation Accuracy: 80.86\n",
            "Epoch:23/100, train_loss:0.007999218918879827, validation Accuracy: 80.2\n",
            "Epoch:24/100, train_loss:0.0077798949261506396, validation Accuracy: 81.54\n",
            "Epoch:25/100, train_loss:0.007677312745319472, validation Accuracy: 81.74\n",
            "Epoch:26/100, train_loss:0.007472540807061725, validation Accuracy: 82.7\n",
            "Epoch:27/100, train_loss:0.00733159780005614, validation Accuracy: 82.48\n",
            "Epoch:28/100, train_loss:0.007302016333076689, validation Accuracy: 81.92\n",
            "Epoch:29/100, train_loss:0.007088444106115235, validation Accuracy: 82.5\n",
            "Epoch:30/100, train_loss:0.006987905989752875, validation Accuracy: 83.4\n",
            "Epoch:31/100, train_loss:0.006902873219715225, validation Accuracy: 83.74\n",
            "Epoch:32/100, train_loss:0.006790501400166088, validation Accuracy: 83.28\n",
            "Epoch:33/100, train_loss:0.006522348242004712, validation Accuracy: 82.94\n",
            "Epoch:34/100, train_loss:0.006494637274742127, validation Accuracy: 83.5\n",
            "Epoch:35/100, train_loss:0.006437035046683417, validation Accuracy: 83.16\n",
            "Epoch:36/100, train_loss:0.006381414822075102, validation Accuracy: 83.2\n",
            "Epoch:37/100, train_loss:0.006319855946302414, validation Accuracy: 83.7\n",
            "Epoch:38/100, train_loss:0.006296003818180826, validation Accuracy: 83.66\n",
            "Epoch:39/100, train_loss:0.006108726843860414, validation Accuracy: 84.46\n",
            "Epoch:40/100, train_loss:0.006085668246282472, validation Accuracy: 83.24\n",
            "Epoch:41/100, train_loss:0.0060465701573424865, validation Accuracy: 83.66\n",
            "Epoch:42/100, train_loss:0.005935839561290211, validation Accuracy: 84.76\n",
            "Epoch:43/100, train_loss:0.005846427362163862, validation Accuracy: 83.86\n",
            "Epoch:44/100, train_loss:0.005812676975462172, validation Accuracy: 83.62\n",
            "Early Stopped!\n",
            "Test Accuracy: 84.03\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}